{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdqglxBqcwO_"
      },
      "source": [
        "*Por favor, imagina que estás trabajando en un proyecto como **Científico de Datos** y se te solicita completar la siguiente información.*\n",
        "\n",
        "## **Información del estudiante**\n",
        "---\n",
        "* **Título**:Clasificación de Tipos de Financiamiento en un Entorno Open Finance mediante Modelos de Machine Learning\n",
        "* **Autor**: Joaquin Ureta Saenz Peña\n",
        "* **Correo**:juretasaenzpena2@uoc.edu\n",
        "* **Salida**: ipynb, predicciones.csv\n",
        "---\n",
        "\n",
        "### **Contexto**:\n",
        "Este ejercicio se basa en el conjunto de datos GFT Open Finance, cuyo objetivo es fomentar la competencia entre proveedores financieros, impulsar la innovación digital y promover nuevos servicios basados en datos. Este conjunto de datos se centra en la banca abierta y ofrece una valiosa oportunidad de aprendizaje sobre datos y tecnologías abiertas en el sector financiero.\n",
        "\n",
        "La actividad propuesta permitirá a los estudiantes enfrentar la nueva realidad del intercambio de información bancaria, confrontando bases de datos de diferentes instituciones financieras, incluidas dos bancos y una compañía de seguros.\n",
        "\n",
        "El objetivo es practicar habilidades de ingeniería de datos y ciencia de datos al desarrollar un sistema que sugiera **la clasificación de los tipos de financiamiento para un cliente específico**. Esto contribuirá a mejorar la experiencia del cliente mediante el diseño de modelos que optimicen la oferta de tipos de financiamiento.\n",
        "\n",
        "Open Finance representa la evolución de Open Banking y promete traer más transparencia y autonomía a los usuarios del sistema financiero. Los datos proporcionados corresponden a la implementación de Open Finance en Brasil, donde se permite el intercambio de datos sobre seguros, inversiones y fondos de pensiones. El conjunto de datos incluye información financiera de un banco minorista (RetailBankEFG) obtenida a través de financiamiento abierto, con el consentimiento de los clientes, así como datos de un banco de inversión (InvestmentBankCDE) y una compañía de seguros (InsuranceCompanyABC). Estos conjuntos de datos contienen datos de compras anteriores de clientes, así como algunos datos demográficos.\n",
        "\n",
        "### **Problema:**\n",
        "El reto consiste en concebir una solución que integre las bases de datos de estas instituciones financieras en el contexto emergente de Open Finance, procese los datos con eficacia y cree modelos de clasificación para tipos de financiamientos utilizando algoritmos de aprendizaje automático.\n",
        "\n",
        "### **Preguntas:**\n",
        "Durante el desarrollo del ejercicio encontrarás una o más secciones de preguntas. Por favor, responde de manera justificada y, si se solicita incluir código, investiga para responder adecuadamente.\n",
        "\n",
        "**Nota:** Para algunas respuestas, debes proporcionar la solución mediante código en Python e implementarla justo debajo de la línea correspondiente.\n",
        "`#Write your code here`\n",
        "\n",
        "### **Restricciones:**\n",
        "No puedes borrar los nombres de las variables propuestas para el desarrollo del ejercicio.\n",
        "\n",
        "### **Actividades:**\n",
        "Para elaborar una solución que consolide las bases de datos de estas instituciones financieras y genere modelos de clasificación de tipos de financiamiento mediante algoritmos de aprendizaje automático, podemos dividir el proceso en los siguientes pasos:\n",
        "\n",
        "- **Preparación de datos:** Integrar y limpiar los datos de las bases de datos de las instituciones financieras, asegurando que estén en un formato adecuado para su análisis.\n",
        "- **Exploración de datos:** Realizar un análisis exploratorio de los datos para comprender mejor su estructura, distribución y características. Esto puede incluir la visualización de datos y la identificación de posibles patrones o relaciones.\n",
        "- **Ingesta de datos:** Integrar las bases de datos de las instituciones financieras en un único repositorio de datos, asegurando que la información se pueda acceder de manera eficiente y segura.\n",
        "- **Procesamiento de datos:** Realizar transformaciones adicionales en los datos según sea necesario, como la codificación de variables categóricas, la normalización de características numéricas y la manipulación de valores faltantes.\n",
        "- Claro, aquí tienes el texto corregido:\n",
        "- **Desarrollo de modelos para clasificar los tipos de financiamiento:** Seleccionar y entrenar algoritmos de clasificación adecuados para el problema de clasificación de tipos de financiamiento. Esto puede incluir técnicas como algoritmos de clasificación supervisada, tales como Support Vector Machines, Random Forests o redes neuronales.\n",
        "- **Validación del modelo:** Evaluar el rendimiento de los modelos utilizando métricas adecuadas, como precisión, recall, F1-score, etc. Utilizar técnicas como la validación cruzada para garantizar la generalización del modelo.\n",
        "- **Optimización del modelo:** Ajustar hiperparámetros y realizar selección de características para optimizar el rendimiento de los modelos.\n",
        "\n",
        "## **Solución**:\n",
        "El presente proyecto aborda un problema de clasificacion multiclase en el contexto de Open Finance, cuyo objetivo es predecir el tipo de financiamiento asociado a un cliente (casa, carro, ambos o ninguno), a partir de la integracion de datos provenientes de distintas instituciones financieras.\n",
        "\n",
        "La solucion desarrollada contempla un proceso completo de ingenieria de datos y ciencia de datos, comenzando por la integracion y limpieza de multiples fuentes de informacion, seguido por un analisis exploratorio detallado, la creacion de nuevas variables, el tratamiento de valores tipicos y desbalance de clases,\n",
        "y finalmente la construccion y evaluacion de distintos modelos de Machine Learning.\n",
        "\n",
        "Se implementaron pipelines de preprocesamiento que permiten asegurar consistencia, reproducibilidad y escalabilidad del flujo de datos, dejando el conjunto final preparado para ser utilizado en modelos predictivos de forma eficiente.\n",
        "\n",
        "## **Entorno**:\n",
        "El desarrollo se realizo en un entorno Python utilizando Jupyter Notebook, con las siguientes librerias principales:\n",
        "- pandas, numpy\n",
        "- matplotlib, seaborn\n",
        "- scikit-learn\n",
        "- imbalanced-learn\n",
        "\n",
        "El uso de pipelines y transformadores personalizados permitió mantener una estructura clara y modular, alineada con buenas prácticas de ciencia de datos.\n",
        "\n",
        "\n",
        "## **Origen de la fuente de datos**:\n",
        "\n",
        "Los datos están ubicados en la carpeta \"data\" y constan de los siguientes archivos.\n",
        "\n",
        "* InvestmentBankCDE.csv\n",
        "* RetailBankEFG.csv\n",
        "* InsuranceCompanyABC.csv\n",
        "\n",
        "Nota: Los datos proporcionados son ficticios y no se corresponden con la realidad de ninguna manera. A continuación, se muestran algunas de las columnas a utilizar por el modelo.\n",
        "\n",
        "*Por favor, agrega aquellas columnas que faltan y que se encuentran en el archivo InsuranceCompanyABC.csv*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "[\n",
        "  \"seguro auto\",\n",
        "  \"seguro vida Emp\",\n",
        "  \"seguro vida PF\",\n",
        "  \"Seguro Residencial\",\n",
        "  \"Investimento Fundos_cambiais\",\n",
        "  \"Investimento Fundos_commodities\",\n",
        "  \"Investimento LCI\",\n",
        "  \"Investimento LCA\",\n",
        "  \"Investimento Poupanca\",\n",
        "  \"Investimento Fundos Multimercado\",\n",
        "  \"Investimento Tesouro Direto\",\n",
        "  \"Financiamento Casa\",\n",
        "  \"Financiamento Carro\",\n",
        "  \"Emprestimo _pessoal\",\n",
        "  \"Emprestimo _consignado\",\n",
        "  \"Emprestimo _limite_especial\",\n",
        "  \"Emprestimo _educacao\",\n",
        "  \"Emprestimo _viagem\",\n",
        "  \"Investimento CDB\",\n",
        "  \"Investimento Fundos\"\n",
        "]\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6MCkBtph8hO"
      },
      "source": [
        "# Librerías\n",
        "Las siguientes son varias de las librerias necesarias para el desarrollo del ejercicio; sin embargo, estas no estan limitadas es decir puedes incluir otras librerias para desarrollar el ejercico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G8OhAHgw0YVz"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'seaborn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KMeans\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    VotingClassifier,\n",
        "    ExtraTreesClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier\n",
        ")\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    learning_curve,\n",
        "    ShuffleSplit,\n",
        "    cross_val_score,\n",
        "    KFold\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score\n",
        ")\n",
        "from functools import reduce\n",
        "from sklearn.preprocessing import (\n",
        "    LabelEncoder,\n",
        "    StandardScaler\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import (\n",
        "    SMOTE,\n",
        "    RandomOverSampler,\n",
        "    SMOTENC,\n",
        "    ADASYN\n",
        ")\n",
        "from imblearn.under_sampling import (\n",
        "    RandomUnderSampler,\n",
        "    NearMiss\n",
        ")\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXocH_MZ5bHc"
      },
      "source": [
        "## Configuración de visualización de conjuntos de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4RmQqJ4ecY9"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jv7D0ZEiAlW"
      },
      "source": [
        "### Descarga las fuentes de datos\n",
        "\n",
        "Si estás utilizando Google Colaboratory o un entorno Linux con la herramienta wget, puedes descomentar las siguientes líneas para descargar los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqhxbV4Xih8i",
        "outputId": "908e003d-e096-4fec-b742-4c98150cd59e"
      },
      "outputs": [],
      "source": [
        "#!mkdir data\n",
        "#!wget https://raw.githubusercontent.com/maratonadev/desafio-3-2021/main/assets/data/InvestmentBankCDE.csv  -O data/InvestmentBankCDE.csv\n",
        "#!wget https://raw.githubusercontent.com/maratonadev/desafio-3-2021/main/assets/data/RetailBankEFG.csv -O data/RetailBankEFG.csv\n",
        "#!wget https://raw.githubusercontent.com/maratonadev/desafio-3-2021/main/assets/data/InsuranceCompanyABC.csv -O data/InsuranceCompanyABC.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1st6AlbPBcE"
      },
      "source": [
        "## Variables\n",
        "No puedes borrar las variables descritas a continuación; sin embargo, puedes incluir tus propias variables si estas te ayudan a responder alguna pregunta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPvws4X_Clp7"
      },
      "outputs": [],
      "source": [
        "df_retailbank = None\n",
        "df_investment = None\n",
        "df_insurance = None\n",
        "data_frame_merged = None\n",
        "data_frame_tipo_financiamiento = None\n",
        "tipo_financiamiento_mapping = None\n",
        "pca_model = None\n",
        "X_principal = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZKTjiVQNGj6"
      },
      "source": [
        "## Funciones\n",
        "A continuación se presentan algunas funciones para gráficar que pueden ser útiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYkktpxmNE_o"
      },
      "outputs": [],
      "source": [
        "def plot_boxplot_violinplot(x, y, data_frame):\n",
        "    \"\"\"\n",
        "    Plot both boxplot and violinplot for comparison.\n",
        "\n",
        "    Parameters:\n",
        "    x (str): The column name for the x-axis.\n",
        "    y (str): The column name for the y-axis.\n",
        "    data_frame (pandas.DataFrame): The DataFrame containing the data.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Set figure size\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(2, 1)\n",
        "\n",
        "    # Rotate x-axis labels\n",
        "    for ax in axes:\n",
        "        ax.tick_params(axis='x', rotation=70)\n",
        "\n",
        "    # Plot violinplot\n",
        "    sns.violinplot(data=data_frame, x=x, y=y, ax=axes[0])\n",
        "\n",
        "    # Plot boxplot\n",
        "    sns.boxplot(data=data_frame, x=x, y=y, ax=axes[1])\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show plots\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2DrUTqrOOMj"
      },
      "outputs": [],
      "source": [
        "def plot_count_plots(df_base, columnas):\n",
        "    \"\"\"\n",
        "    Plot count plots for specified columns.\n",
        "\n",
        "    Parameters:\n",
        "    df_base (pandas.DataFrame): The DataFrame containing the data.\n",
        "    columnas (list): A list of column names to plot.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Determine the number of rows and columns for subplots\n",
        "    num_plots = len(columnas)\n",
        "    num_rows = (num_plots + 1) // 2  # Ensure there's at least one row\n",
        "    num_cols = min(2, num_plots)  # Maximum of 2 columns\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
        "\n",
        "    # Flatten axes if necessary\n",
        "    if num_plots == 1:\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    # Iterate through each column\n",
        "    for columna, ax in zip(columnas, axes):\n",
        "        # Plot count plot\n",
        "        sns.countplot(x=columna, data=df_base, ax=ax)\n",
        "        ax.set_title(f'Count Plot for {columna}')  # Add title\n",
        "        ax.tick_params(axis='x', rotation=90)  # Rotate x-axis labels\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxf6ShJNoAob"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, mapping, title='Confusion matrix', cmap=None, normalize=True):\n",
        "    # Calculate accuracy and misclassification rate\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    # Set default color map if not provided\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    # Create a new figure\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Display confusion matrix as image\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    labes = [mapping[key] for key in mapping.keys() ]\n",
        "    print(\"mapping\",mapping)\n",
        "    # Display target names on ticks if provided\n",
        "    if labes is not None:\n",
        "        tick_marks = np.arange(len(labes))\n",
        "        plt.xticks(tick_marks, labes, rotation=45)\n",
        "        plt.yticks(tick_marks, labes)\n",
        "\n",
        "    # Normalize confusion matrix if required\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Set threshold for text color based on normalization\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "\n",
        "    # Add text annotations to cells\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # Set labels for axes\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk4IWSy4oUWq"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy_scores(estimator, train_x, train_y, test_x, test_y, nparts=5, jobs=None):\n",
        "    # Initialize KFold with specified number of splits, shuffling, and random state\n",
        "    kfold = KFold(n_splits=nparts, shuffle=True, random_state=123)\n",
        "\n",
        "    # Create a new figure and axes for the plot\n",
        "    fig, axes = plt.subplots(figsize=(7, 3))\n",
        "\n",
        "    # Set plot title and labels for x and y axes\n",
        "    axes.set_title(\"Accuracy Ratio / Fold Number\")\n",
        "    axes.set_xlabel(\"Fold Number\")\n",
        "    axes.set_ylabel(\"Accuracy\")\n",
        "\n",
        "    # Compute accuracy scores for training data using cross-validation\n",
        "    train_scores = cross_val_score(estimator, train_x, train_y, cv=kfold, n_jobs=jobs, scoring=\"accuracy\")\n",
        "\n",
        "    # Compute accuracy scores for test data using cross-validation\n",
        "    test_scores = cross_val_score(estimator, test_x, test_y, cv=kfold, n_jobs=jobs, scoring=\"accuracy\")\n",
        "\n",
        "    # Generate sequence of fold numbers\n",
        "    train_sizes = range(1, nparts+1, 1)\n",
        "\n",
        "    # Add grid lines to the plot\n",
        "    axes.grid()\n",
        "\n",
        "    # Plot accuracy scores for training data\n",
        "    axes.plot(train_sizes, train_scores, 'o-', color=\"r\", label=\"Training Data\")\n",
        "\n",
        "    # Plot accuracy scores for cross-validation data\n",
        "    axes.plot(train_sizes, test_scores, 'o-', color=\"g\", label=\"Cross-Validation\")\n",
        "\n",
        "    # Add legend to the plot\n",
        "    axes.legend(loc=\"best\")\n",
        "\n",
        "    # Return the accuracy scores for training data\n",
        "    return train_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "340dNgw4qX8E"
      },
      "outputs": [],
      "source": [
        "def startified_train_test_split(X, Y, n_splits=1, test_size=0.2, random_state=42):\n",
        "  # Assuming X and y are your feature matrix and target variable respectively\n",
        "\n",
        "  # Initialize StratifiedShuffleSplit\n",
        "  stratified_splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
        "\n",
        "  # Split the data while maintaining the class distribution\n",
        "  for train_index, test_index in stratified_splitter.split(X, y):\n",
        "      X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "  return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ychORzExSiJQ"
      },
      "outputs": [],
      "source": [
        "def plot_pca_cumulative_variance(pca):\n",
        "    \"\"\"\n",
        "    Plot the cumulative explained variance of principal components.\n",
        "\n",
        "    Parameters:\n",
        "    pca (PCA): The fitted PCA object.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Determine explained variance using explained_variance_ration_ attribute\n",
        "    exp_var_pca = pca.explained_variance_ratio_\n",
        "\n",
        "    # Cumulative sum of eigenvalues; This will be used to create step plot\n",
        "    # for visualizing the variance explained by each principal component.\n",
        "    cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
        "\n",
        "    # Create the visualization plot\n",
        "    plt.bar(range(1,len(exp_var_pca)+1), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
        "    plt.step(range(1,len(cum_sum_eigenvalues)+1), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
        "    plt.ylabel('Explained variance ratio')\n",
        "    plt.xlabel('Principal component index')\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFYmD_ZrTzXq"
      },
      "outputs": [],
      "source": [
        "def get_pca_components(pca, columns):\n",
        "    # Number of components\n",
        "    n_pcs = pca.components_.shape[0]\n",
        "\n",
        "    # Get the index of the most important feature on EACH component i.e. largest absolute value\n",
        "    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
        "\n",
        "    # Initial feature names\n",
        "    initial_feature_names = columns\n",
        "\n",
        "    # Get the names\n",
        "    most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
        "\n",
        "    # Using dictionary comprehension to create a dictionary\n",
        "    dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
        "\n",
        "    # Return a DataFrame sorted by keys\n",
        "    return pd.DataFrame(sorted(dic.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om0CMJA6X99t"
      },
      "outputs": [],
      "source": [
        "def plot_elbow_curve_pca(X_principal):\n",
        "    \"\"\"\n",
        "    Plot the elbow curve for PCA.\n",
        "\n",
        "    Parameters:\n",
        "    X_principal (DataFrame): Transformed features into principal components.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    ks = range(1, 10)\n",
        "    inertias = []\n",
        "\n",
        "    for k in ks:\n",
        "        # Create a KMeans instance with k clusters: model\n",
        "        model = KMeans(n_clusters=k, n_init=\"auto\")\n",
        "\n",
        "        # Fit model to samples\n",
        "        model.fit(X_principal)\n",
        "\n",
        "        # Append the inertia to the list of inertias\n",
        "        inertias.append(model.inertia_)\n",
        "\n",
        "    # Plot ks vs inertias\n",
        "    plt.plot(ks, inertias, '-o')\n",
        "    plt.xlabel('Number of clusters, k')\n",
        "    plt.ylabel('Inertia')\n",
        "    plt.xticks(ks)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb2iydgA6_OW"
      },
      "source": [
        "# **Pregunta 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H6hoQw_Cubn"
      },
      "source": [
        "# Preparación de datos\n",
        "\n",
        "La preparación de datos es un paso crucial en el proceso de análisis de datos. Asegura que los datos sean precisos, consistentes y utilizables para análisis posteriores. A continuación, se presentan los pasos que vamos a seguir para realizar una limpieza de datos efectiva:\n",
        "\n",
        "1. **Entender los Datos**\n",
        "   - **Recopilar Información**: Conocer la fuente de los datos, cómo se recolectaron y su estructura.\n",
        "   - **Exploración Inicial**: Examinar los datos para entender su contenido, formato y posibles problemas.\n",
        "\n",
        "2. **Evaluación de Calidad de Datos**\n",
        "   - **Valores Faltantes:** Identificar valores nulos o faltantes en el conjunto de datos.\n",
        "   - **Duplicados:** Detectar filas duplicadas que pueden distorsionar los análisis.\n",
        "   - **Inconsistencias:** Buscar inconsistencias en los datos, como diferentes formatos de fechas o variaciones en la nomenclatura.\n",
        "\n",
        "3. **Ingeniería de características:**\n",
        "   - **Transformaciones:** Muchas técnicas de ingeniería de características, como la creación de términos de interacción, características polinómicas o agregaciones, son más significativas e interpretables en la escala original de los datos.\n",
        "\n",
        "4. **Limpieza de Datos**\n",
        "   - **Manejo de Valores Faltantes**\n",
        "      - **Eliminar:** Remover filas o columnas con valores faltantes si son pocas y no impactan el análisis.\n",
        "      - **Imputar:** Rellenar valores faltantes usando métodos como la media, mediana, moda o técnicas más avanzadas como la imputación con modelos predictivos.\n",
        "   - **Eliminación de Duplicados**\n",
        "      - **Identificar y Eliminar:** Usar herramientas para detectar y remover filas duplicadas.\n",
        "   - **Corrección de Inconsistencias**\n",
        "      - **Estándar de Formato:** Uniformizar formatos de fechas, texto, etc.\n",
        "      - **Reemplazar Valores Erróneos:** Corregir errores tipográficos y valores fuera de rango.\n",
        "   - **Normalización y Escalado**\n",
        "      - **Normalización:** Convertir datos a una escala común.\n",
        "      - **Escalado:** Ajustar los valores para que estén dentro de un rango específico, útil para algoritmos de machine learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAVcaTbAHZSN"
      },
      "source": [
        "# Entender los Datos\n",
        "##Recopilar Información\n",
        "## Preguntas\n",
        "* *¿Cuáles son los desafíos clave al integrar y analizar datos de diferentes instituciones financieras para desarrollar sistemas de recomendación de seguros?*\n",
        "\n",
        "Los principales desafios incluyen la heterogeneidad de formatos, diferencais semanticas entre variables, posibles inconsistencias de clientes, presencia de valores faltantes y desbalance de informacion entre fuentes. Estos factores pueden impactar negativamnete la calidad del analisis si no se \n",
        "abordan adecuadamente durante la etapa de preparacion de datos.\n",
        "\n",
        "* *¿De qué manera podría su participación en el desarrollo de nuevas fuentes de información de seguros en el marco de Open Finance promover la transparencia y autonomía de los usuarios del sistema financiero?*\n",
        "\n",
        "Open Finance permite que los usuarios tengan control sobre sus propios datos financieros, habilitando su interccambio entre instituciones de forma seguro y consentida. Esto fomenta mayor transparencia, mejores ofertas personalizadas y una mayor competencia entre proveedores financieros.\n",
        "\n",
        "* *¿Cuál es la similitud entre Open Finance y otras fuentes de datos financieros abiertos, como Open Banking y Open Insurance, y cómo benefician a los usuarios del sistema financiero en términos de transparencia y acceso a información?*\n",
        "\n",
        "Los tres comparten el principio de interoperabilidad y acceso abierto a los daos, con el objetivo de empoderar al usuario final. Open Finance amplia este concepto integrando no solo banca, sino tambien seguros e inversiones, ofreciendo una vision financiera mas completa\n",
        "\n",
        "* *¿Qué aspectos clave deberías revisar al explorar los datos de GFT Open Finance para entender su contenido, formato y posibles problemas, y cómo estos podrían afectar el desarrollo de modelos de machine learning para recomendaciones de seguros?*\n",
        "\n",
        "Es fundamental revisar:\n",
        "- calidad y consistencia de los datos\n",
        "- Distribucion de variables\n",
        "- Presencia de valores faltantes y duplicados\n",
        "- Formatos incorrectos o valores atipicos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4qNVFYw2KNX"
      },
      "source": [
        "## Exploración Inicial\n",
        "\n",
        "Comencemos importando los diferentes conjuntos de datos como dataframes utilizando la librería de pandas. Luego, procederemos a presentar los primeros 10 registros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "mJNDgAV8L4hH",
        "outputId": "d3e0b218-add6-41a9-f80d-72cb3448d491"
      },
      "outputs": [],
      "source": [
        "#Write your code here\n",
        "df_retailbank = pd.read_csv(\"data/RetailBankEFG.csv\")\n",
        "df_retailbank.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gjr_Zp37UXJ"
      },
      "source": [
        "*Realiza la misma acción para InvestmentBankCDE.csv.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "UBRtN41WzMGO",
        "outputId": "d4d21e50-274f-492d-a25d-64af03a3ada3"
      },
      "outputs": [],
      "source": [
        "#Write your code here\n",
        "df_investment = pd.read_csv(\"data/InvestmentBanckCDE.csv\")\n",
        "df_investment.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JuAB43V7i6s"
      },
      "source": [
        "*Realiza la misma acción para InvestmentBankCDE.csv.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "YyMSYFv2MTRP",
        "outputId": "0fe535ed-5815-4b42-aa1b-fc3510b8e2fa"
      },
      "outputs": [],
      "source": [
        "#Write your code here\n",
        "df_insurance = pd.read_csv(\"data//InsuranceCompanyABC.csv\")\n",
        "df_insurance.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4etqHm5cMhLE"
      },
      "source": [
        "## Pregunta\n",
        "*¿Puedes identificar un atributo común entre los diferentes conjuntos de datos que permita juntarlos?*\n",
        "Si, el atributo comun entre los tres conjuntos de datos es la columna ID, la cual identifica de forma unica a cada cliente. Este identificador permite integrar los datos provenientes de las distintas instituciones financieras en un unico DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueWGipEHMCTK",
        "outputId": "7ebd986a-3c52-41ba-ec30-b42bcf8a0191"
      },
      "outputs": [],
      "source": [
        "#Write your code here\n",
        "common_columns = (\n",
        "    set(df_retailbank.columns)\n",
        "    &set(df_investment.columns)\n",
        "    &set(df_insurance.columns)\n",
        ")\n",
        "common_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAS6OEfHXYKx"
      },
      "source": [
        "## Pregunta\n",
        "Indica cuál es la cantidad de registros en cada conjunto de datos.\n",
        "\n",
        "*¿Qué conclusiones puedes sacar luego de observar los resultados?*\n",
        "Los tres conjuntos de datos cuentas con la misma cantidad de registros. Esto sugiere que todos represnetan al mismo universo de clieentes, pero desde distintas perspectivas (bancaria, de inversion y de seguros). Esta consistencia facilita el proceso\n",
        "de integracion de los datos y reduce el riesgo de perdida de informaicon al realizar el merge entre los datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VJrTyZ1Ogkl",
        "outputId": "f4a5b417-49b5-4b52-81e3-a8120aaf046f"
      },
      "outputs": [],
      "source": [
        "#Write your code here\n",
        "print(\"RetailBankEFG:\", df_retailbank.shape[0])\n",
        "print(\"InvestmentBankCDE:\", df_investment.shape[0])\n",
        "print(\"InsuranceCompanyABC:\", df_insurance.shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWbyVZAsee_e"
      },
      "source": [
        "## Pregunta\n",
        "¿Has notado algún patrón entre los datos, ya sea entre filas o columnas?\n",
        "A partir de la exploración inicial, se observa que cada fila representa a un cliente único identificado por la columna ID, mientras que las columnas describen distintas características financieras, demográficas y de productos. Además, se aprecia una separación clara entre variables numéricas y categóricas, lo que indica que será necesario aplicar técnicas de codificación y escalado en etapas posteriores. También se intuye que algunas variables pueden estar relacionadas entre sí, como edad e ingresos, lo que podría ser relevante para los modelos de machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC7a3j4VX3Vm"
      },
      "source": [
        "# Evaluación de Calidad de Datos\n",
        "\n",
        "## Valores Faltantes:\n",
        "Vamos a identificar los valores nulos o faltantes en los conjuntos de datos. Para esto, crearás una función llamada `get_nan_values`. Esta función tomará como parámetro un dataframe y devolverá el número de valores nulos por fila y por columna.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pi3tCafYe0t"
      },
      "outputs": [],
      "source": [
        "def get_nan_values(data_frame):\n",
        "  # Count NaN values in each column\n",
        "  nan_count_per_column = data_frame.isna().sum()\n",
        "  # Total number of records with NaN values\n",
        "  total_nan_records = data_frame.isna().any(axis=1).sum()\n",
        "  # pass\n",
        "  return {\"Count NaN values in each column\":nan_count_per_column,\"Total number of records with NaN values\":total_nan_records}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7eVtYjt9q79"
      },
      "source": [
        "*Imprime los valores faltantes por fila y columna*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0WCNeyG9Gft",
        "outputId": "1c553783-ff05-4e11-e78c-2e0dd78c86a5"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_retailbank\n",
        "print(\"Valores NaN en df_retailbank:\")\n",
        "print(get_nan_values(df_retailbank))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpTzENLiYEA9",
        "outputId": "84f05f0d-fbd1-47df-b9f5-7f7df5e07ce1"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_investment\n",
        "print(\"\\nValores NaN en df_investment:\")\n",
        "print(get_nan_values(df_investment))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ29uEyJaGlo",
        "outputId": "5c626d75-ce34-4701-aeec-6fbca9effaa9"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_insurance\n",
        "print(\"\\nValores NaN en df_insurance:\")\n",
        "print(get_nan_values(df_insurance))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmXTsmVZ0Y2Z"
      },
      "source": [
        "## Pregunta\n",
        "*¿Existen valores faltantes en los datos?*\n",
        "A partir de la evaluacion realizada con la funcion get_nan_values, no se obervan valores faltantes en ninguno de los tres conjuntos de datos, ya que el conteo de NaN por columna y el numero totoal de registros con NaN es igual a 0. Por lo tanto, no es necesario aplicar tecnicas de imputacion en esta etapa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgTlhHXN5a6S"
      },
      "source": [
        "## Duplicados\n",
        "Vamos a detectar si existen filas duplicadas que pueden distorsionar los análisis. Para ello, vamos a validar si hay registros duplicados en el conjunto de datos utilizando la función `check_duplicates`. En caso afirmativo, necesitaremos pasar como parámetros el dataframe a validar y la columna que se utiliza como identificador.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4a32GJW6Uns"
      },
      "outputs": [],
      "source": [
        "def check_duplicates(data_frame, column):\n",
        "    \"\"\"\n",
        "    Check the number of duplicate values in the specified column(s) of a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    data_frame (pandas.DataFrame): The DataFrame to check for duplicates.\n",
        "    column (str or list): The column name or list of column names to check for duplicates.\n",
        "\n",
        "    Returns:\n",
        "    int: The number of duplicate rows.\n",
        "    \"\"\"\n",
        "    # Check for duplicates\n",
        "    duplicates_by_id = data_frame.duplicated(subset=column)\n",
        "\n",
        "    # Count the number of duplicate rows\n",
        "    num_duplicates = duplicates_by_id.sum()\n",
        "\n",
        "    return num_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuLJ4Jg48jfz"
      },
      "source": [
        "*Imprime la cantidad de filas duplicadas para df_retailbank, df_investment y df_insurance*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwA6-aiP7X8b",
        "outputId": "421829df-2fae-4c74-9eac-8c8a276aab76"
      },
      "outputs": [],
      "source": [
        "#Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7TYN089i-VC"
      },
      "source": [
        "## Pregunta\n",
        "¿Existen datos duplicados?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81HL0vDYcwq_"
      },
      "source": [
        "## Inconsistencias\n",
        "En esta sección, se propondrán varios métodos para identificar inconsistencias en los datos. Primero, vamos a revisar las estadísticas básicas. Para ello, utilizaremos la función `describe()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nlxQGQp-XUa"
      },
      "source": [
        "*Imprime las estadísticas básicas*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ueMm4OuWdoYL",
        "outputId": "25bb8b1d-d9ac-409e-8b76-1fb5b9182f97"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_retailbank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "_aIM_RcBd7oB",
        "outputId": "a1679eca-b2d9-4999-86df-7310dcc6bbd5"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_investment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "4_wfqyrVd-4Y",
        "outputId": "cad9dd16-d017-4fb3-de39-7c91e2558af8"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_insurance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0QhsNwHewVH"
      },
      "source": [
        "### Identificar Valores Únicos:\n",
        "Ahora, para todas las variables no numéricas, debemos identificar cuántos tipos de datos están registrados en cada columna. Implementaremos la función `get_value_counts_non_numeric_columns`, la cual obtiene los conteos de valores de las columnas no numéricas en un DataFrame y devuelve un diccionario donde las claves son los nombres de las columnas no numéricas y los valores son sus respectivos conteos de valores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtfa6hxhfARw"
      },
      "outputs": [],
      "source": [
        "def find_non_numeric_columns(df):\n",
        "    \"\"\"\n",
        "    Find non-numeric columns in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): The DataFrame to search for non-numeric columns.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of non-numeric column names.\n",
        "    \"\"\"\n",
        "    return df.select_dtypes(exclude=['number']).columns.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHJiQp6b_d0V"
      },
      "source": [
        "*Implementa la función `get_value_counts_non_numeric_columns`, la cual debe hacer uso de la función `find_non_numeric_columns`. Esta función devuelve un diccionario donde las claves son los nombres de las columnas no numéricas y los valores son sus respectivos conteos de valores.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SszcSoy6_dVo"
      },
      "outputs": [],
      "source": [
        "def get_value_counts_non_numeric_columns(df):\n",
        "    \"\"\"\n",
        "    Get the value counts of non-numeric columns in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): The DataFrame to analyze.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary where keys are non-numeric column names and values are their respective value counts.\n",
        "    \"\"\"\n",
        "    # write your code here\n",
        "    #Get non-numeric columns\n",
        "    #pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkCCBzuE-6QE"
      },
      "source": [
        "*Imprime los conteos de las columnas no numéricas.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msb8QWLFfT-D",
        "outputId": "2e3419a0-63c1-49fe-d5cf-db3bd849312d"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_retailbank\n",
        "get_value_counts_non_numeric_columns(df_retailbank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAjIVNASgJtF",
        "outputId": "0b8535cf-a99e-4feb-88a7-863e267729d8"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_investment\n",
        "get_value_counts_non_numeric_columns(df_investment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGmRCkwhgNQA",
        "outputId": "42b8a072-7cf3-426f-b8ee-5b29caaa367e"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_insurance\n",
        "get_value_counts_non_numeric_columns(df_insurance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QazpT16DhRlv"
      },
      "source": [
        "### Verificar Tipos de Datos:\n",
        "*Utiliza el atributo `dtypes` para verificar los tipos de datos de cada columna.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzQVVfiYhVBT",
        "outputId": "0ef07c3d-f99a-4e96-a276-a901774b1993"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_retailbank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv8e8yTphZvE",
        "outputId": "d7dd2422-2928-4789-d3a3-b21aa07b629f"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_investment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4WjUUDthbmN",
        "outputId": "77e31462-621c-4550-d57a-6e9261865b8f"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_insurance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ6lMtxrhlpv"
      },
      "source": [
        "## Pregunta\n",
        "*¿Qué puedes concluir respecto de todas las variables que no son numéricas?*\n",
        "*¿Has identificado algún patrón o característica?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39UL9mXAjpP_"
      },
      "source": [
        "## Visualización General de los datos y Analizar Patrones Anómalos\n",
        "Esta es una sección libre en la que podrás crear diferentes visualizaciones de los datos. Sugiero que utilices principalmente visualizaciones para validar la cantidad de datos de las variables no numéricas. Además, debes realizar gráficas tipo box plot para las columnas numéricas, exceptuando la columna ID.\n",
        "\n",
        "## Por ejemplo:\n",
        "### Visualizaciones para variables no numéricas:\n",
        "- **Gráfico de barras:** Utiliza un gráfico de barras para visualizar la cantidad de datos únicos en cada variable no numérica.\n",
        "- **Gráfico de pastel:** Muestra la distribución de los datos en cada variable no numérica utilizando un gráfico de pastel.\n",
        "\n",
        "### Box plots para columnas numéricas:\n",
        "- **Box plot para cada columna numérica (excluyendo la columna ID):** Utiliza box plots para visualizar la distribución de los datos, los valores atípicos y la mediana en cada columna numérica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xURK_B-CmKV1",
        "outputId": "28372fcc-0160-4a7f-9056-3ee384225367"
      },
      "outputs": [],
      "source": [
        "#Write your code here, add your custom plots for df_retailbank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CVmTCV3Hl14V",
        "outputId": "e1a914c2-ffd0-4bde-c3a4-2f01bb120fec"
      },
      "outputs": [],
      "source": [
        "#Write your code here, add your custom plots for df_investment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EmBdG0itmDy7",
        "outputId": "b1389280-fec4-4382-9261-d0b615f55cf3"
      },
      "outputs": [],
      "source": [
        "#Write your code here, add your custom plots for df_insurance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbg4Au1TP5pa"
      },
      "source": [
        "## Preguntas\n",
        "1. *¿Cuál de las dos opciones sugieres utilizar para evaluar datos no numéricos: imprimir los valores o crear visualizaciones?*\n",
        "2. *¿Qué otros tipos de visualizaciones se te ocurren que podrías sugerir? Justifica tu respuesta.*\n",
        "3. *¿Existe un desbalance en los datos, es decir, existen más tipos que corresponden a una clase? ¿Cuál es la clase y cómo crees que esto puede afectar al construir modelos de machine learning?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-qC6QdtnOTS"
      },
      "source": [
        "### Analizar Patrones Anómalos:\n",
        "Para realizar el análisis de patrones anómalos, utilizarás la función `plot_boxplot_violinplot`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O1x6nEeQ9bw"
      },
      "source": [
        " *Graficar la región(Regiao) en función de la edad(Idade), del conjunto de datos `df_insurance`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "XFx8iivMnO85",
        "outputId": "6b5d460a-2f68-4db0-e4a9-740d02ceaad3"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_insurance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGx280PARIGu"
      },
      "source": [
        " *Graficar la región(Regiao) en función de la edad(Renda), del conjunto de datos `df_insurance`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "Z56bihThRifp",
        "outputId": "ed3f5cc6-a237-4dea-8b93-44ca64672f4c"
      },
      "outputs": [],
      "source": [
        "#Write your code here for df_insurance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC6YDc3VQoAW"
      },
      "source": [
        "## Preguntas\n",
        "* *¿Cuál es la distribución de datos sugerida?*\n",
        "* *¿Existen datos atípicos en el conjunto de datos?* *¿Cómo podrías corregir estos datos? Justifica tu respuesta*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3XEPl5iQZ0w"
      },
      "source": [
        "# **Pregunta 2 - Limpieza y tratamiento de Datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pWQ1uWlSwEk"
      },
      "source": [
        "# Limpieza de Datos\n",
        "\n",
        "## Manejo de Valores Faltantes\n",
        "\n",
        "### Preguntas\n",
        "1. *¿Luego de la evaluación es necesario realizar alguna técnica para completar datos faltantes?*\n",
        "2. *¿Debemos realizar tareas de imputación de valores luego de analizar los datos?*\n",
        "3. *¿Por favor, describe al menos dos técnicas de imputación de datos para valores faltantes basadas en métodos estadísticos?*\n",
        "4. *¿Por favor, describe al menos dos técnicas de imputación de datos para valores faltantes basadas en métodos predictivos?*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36M9JP3B_noK"
      },
      "source": [
        "## Eliminación de Duplicados\n",
        "\n",
        "### Identificación y Eliminación:\n",
        "\n",
        "*Vamos a eliminar los datos duplicados en todos los conjuntos de datos utilizando la función `drop_duplicates`, junto con el parámetro `inplace`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94soaZhC5dpA"
      },
      "outputs": [],
      "source": [
        "#Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWSGUNpUBURi"
      },
      "source": [
        "## Pregunta\n",
        "\n",
        "*¿Por qué es importante llevar a cabo la tarea de eliminación de duplicados? Por favor, justifica tu respuesta.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfKvRTPpgURR"
      },
      "source": [
        "# Ingeniería de características\n",
        "\n",
        "## Transformaciones\n",
        "Las operaciones de ingeniería de características a menudo dependen de las relaciones entre las características, las cuales pueden distorsionarse al normalizar los datos. Luego, crear nuevas características como identificar los rangos de edades (Idade) y de ingresos (Renda) tiene más sentido en este punto. A continuación, se presenta un ejemplo al crear una nueva clase `CreateNewRangesColumns`, la cual implementa las clases y librerías necesarias para crear estas nuevas características."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxZ2L1PWgU8O"
      },
      "outputs": [],
      "source": [
        "class CreateNewRangesColumns(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # No adjustments needed in fit, simply return the object unchanged\n",
        "        return self\n",
        "\n",
        "    def createAgeRange(self, base_df):\n",
        "        # Extract the age series from the base DataFrame\n",
        "        age_series_temp = base_df['Idade']\n",
        "\n",
        "        # Define conditions to create age ranges\n",
        "        conditions  = [\n",
        "            (age_series_temp >= 0) & (age_series_temp < 25),\n",
        "            (age_series_temp >= 25) & (age_series_temp < 30),\n",
        "            (age_series_temp >= 30) & (age_series_temp < 35),\n",
        "            (age_series_temp >= 35) & (age_series_temp < 40),\n",
        "            (age_series_temp >= 40) & (age_series_temp < 45),\n",
        "            (age_series_temp >= 45) & (age_series_temp < 50),\n",
        "            (age_series_temp >= 50) & (age_series_temp < 55),\n",
        "            (age_series_temp >= 55) & (age_series_temp < 60),\n",
        "            (age_series_temp >= 60)\n",
        "        ]\n",
        "\n",
        "        # Define choices for age ranges\n",
        "        choices = [\n",
        "            'R1-0-24', 'R2-25-29', 'R3-30-34', 'R4-35-39', 'R5-40-44', 'R6-45-49', 'R7-50-54', 'R8-55-59', 'R9-60'\n",
        "        ]\n",
        "\n",
        "        # Create 'AGE_RANGE' column based on defined conditions and choices\n",
        "        base_df['AGE_RANGE'] = np.select(conditions, choices, default=\"UNKNOWN\")\n",
        "\n",
        "        return base_df\n",
        "\n",
        "    def createIncomeRange(self, base_df):\n",
        "        # Convert income series to numeric format\n",
        "        income_series_temp = pd.to_numeric(base_df['Renda'], errors='coerce')\n",
        "\n",
        "        # Define conditions to create income ranges\n",
        "        conditions  = [\n",
        "            (income_series_temp <= 6000),\n",
        "            (income_series_temp >= 6000) & (income_series_temp < 6500),\n",
        "            (income_series_temp >= 6500) & (income_series_temp < 7000),\n",
        "            (income_series_temp >= 7000) & (income_series_temp < 7500),\n",
        "            (income_series_temp >= 7500) & (income_series_temp < 8000),\n",
        "            (income_series_temp >= 8000) & (income_series_temp < 8500),\n",
        "            (income_series_temp >= 8500) & (income_series_temp < 9000),\n",
        "            (income_series_temp >= 9000)\n",
        "        ]\n",
        "\n",
        "        # Define choices for income ranges\n",
        "        choices = [\n",
        "            'R1-6000', 'R2-6000-6500', 'R3-6500-7000', 'R4-7000-7500', 'R5-7500-8000', 'R6-8000-8500', 'R7-8500-9000', 'R8-9000'\n",
        "        ]\n",
        "\n",
        "        # Create 'INCOME_RANGE' column based on defined conditions and choices\n",
        "        base_df['INCOME_RANGE'] = np.select(conditions, choices, default=\"UNKNOWN\")\n",
        "\n",
        "        return base_df\n",
        "\n",
        "    def transform(self, X):\n",
        "        # First, make a copy of the input DataFrame 'X'\n",
        "        data = X.copy()\n",
        "\n",
        "        # Create the age range column\n",
        "        df_with_age_range = self.createAgeRange(data)\n",
        "\n",
        "        # Create the income range column\n",
        "        df_with_income_range = self.createIncomeRange(df_with_age_range)\n",
        "\n",
        "        return df_with_income_range"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ-0F3yRioaM"
      },
      "source": [
        "A continuación, te presentamos un ejemplo de cómo utilizar esta clase(`CreateNewRangesColumns`). Después, podrás observar que el DataFrame `df_insurance` ahora cuenta con dos nuevas columnas: `AGE_RANGE` e `INCOME_RANGE`, las cuales contienen la información de la identificación de nuevos grupos de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "OsoFmdlEinc0",
        "outputId": "5ddbfbcb-b311-4127-b7aa-7e92fd30bb94"
      },
      "outputs": [],
      "source": [
        "df_insurance = CreateNewRangesColumns().fit_transform(df_insurance)\n",
        "df_insurance.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS5ncmHexGcn"
      },
      "source": [
        "Imprimimos las nuevas columnas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW62ZUdKuumM",
        "outputId": "6f7f5de9-042d-4153-fe70-544f2298ae7f"
      },
      "outputs": [],
      "source": [
        "df_insurance.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEBcvUEJTsca"
      },
      "source": [
        "Ahora visualizamos las nuevas escalas de los datos mediante un gráfico de barras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ipnMWsdlTg6i",
        "outputId": "a1eff29d-f8cf-4419-af1e-c25e9f1e81dc"
      },
      "outputs": [],
      "source": [
        "plot_count_plots(df_insurance,[\"AGE_RANGE\",\"INCOME_RANGE\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhbGML6BUbSf"
      },
      "source": [
        "## Corrección de Inconsistencias\n",
        "\n",
        "### Estándar de Formato:\n",
        "A continuación vamos a normalizar los datos numéricos. Luego convertiremos las variables categóricas de Falso (F) y Verdadero (T) a valores numéricos binarios: 0 para Falso y 1 para Verdadero.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHa57qEz3v4f"
      },
      "source": [
        "### Reemplazar Valores Erróneos:\n",
        "Como pudiste observar en las gráficas anteriores, existen diferentes valores atípicos que se encuentran fuera del rango. Para abordar esto, vamos a crear una opción que nos permita excluir los datos atípicos de nuestro conjunto de datos `df_insurance`. Para tomar esta decisión, eliminaremos todos los registros que sean menores al primer cuartil y todos aquellos mayores al tercer cuartil. El resultado final se asignará al DataFrame `df_insurance`. Para llevar a cabo este proceso, haremos uso de la clase `OutlierRemover`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBM_xATAmKQr"
      },
      "outputs": [],
      "source": [
        "# Custom transformer to remove outliers from specified columns\n",
        "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=1.5, columns=None):\n",
        "        # Initialize with a threshold and list of columns to check for outliers\n",
        "        self.threshold = threshold\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # No fitting necessary for outlier detection based on IQR\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        # Convert to DataFrame if necessary for easier manipulation\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X = pd.DataFrame(X)\n",
        "\n",
        "        # If no specific columns are provided, use all columns\n",
        "        if self.columns is None:\n",
        "            self.columns = X.columns\n",
        "\n",
        "        # Calculate the 1st (Q1) and 3rd (Q3) quartiles for specified columns\n",
        "        Q1 = X[self.columns].quantile(0.25)\n",
        "        Q3 = X[self.columns].quantile(0.75)\n",
        "        # Calculate the interquartile range (IQR)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        # Define the lower and upper bounds for detecting outliers\n",
        "        lower_bound = Q1 - self.threshold * IQR\n",
        "        upper_bound = Q3 + self.threshold * IQR\n",
        "\n",
        "        # Create a mask to identify rows with any feature values outside the bounds\n",
        "        mask = ((X[self.columns] >= lower_bound) & (X[self.columns] <= upper_bound)).all(axis=1)\n",
        "\n",
        "        # Keep only the rows that are within the bounds\n",
        "        X_filtered = X[mask].reset_index(drop=True)\n",
        "\n",
        "        # Replace the original specified columns with the filtered values\n",
        "        X[self.columns] = X_filtered[self.columns]\n",
        "\n",
        "        return X.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1e1yi7lVRBo"
      },
      "source": [
        "*Ejecuta la transformación utilizando la clase `OutlierRemover` y asigna el resultado a `df_insurance`*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "hyTz7vLV77MF",
        "outputId": "b0cc669f-6cf8-45e4-9ab1-673a29741281"
      },
      "outputs": [],
      "source": [
        "#Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRXMchx7Rgzz"
      },
      "source": [
        "## Pregunta\n",
        "Después de eliminar los datos atípicos, ¿cuántos registros tiene ahora el DataFrame `df_insurance`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f36kWA58A-3",
        "outputId": "783c8dfb-f114-4d83-9d96-4adea51d4cd7"
      },
      "outputs": [],
      "source": [
        "#Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svcFrN4cUOKO"
      },
      "source": [
        "## Pregunta\n",
        "*Explica con tus propias palabras cómo podría afectar una diferencia significativa en el tamaño del conjunto de datos antes y después de eliminar los valores atípicos. ¿Qué implicaciones podría tener esto en los resultados de un modelo de machine learning?*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fjFM9ZdSMGu"
      },
      "source": [
        "## Gráficos luego de eliminar datos atípicos\n",
        "\n",
        "En las siguientes gráficas, puedes observar las diferencias con respecto a las del apartado inicial \"Analizar Patrones Anómalos\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvrijqycx1LP"
      },
      "source": [
        " *Graficar la región(Regiao) en función de la edad(Idade), del conjunto de datos `df_insurance`, utilizando la función `plot_boxplot_violinplot`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "xXTwg2IPOihy",
        "outputId": "85951968-7c8e-4b1b-d9a0-7ecbbf3a24c9"
      },
      "outputs": [],
      "source": [
        "#Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w6OgJoSJOX8"
      },
      "source": [
        " *Graficar la región(Regiao) en función de los ingresos(Renda), del conjunto de datos `df_insurance`, utilizando la función `plot_boxplot_violinplot`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "6Oce1cwrOmNJ",
        "outputId": "89e5265f-db1e-4a50-def7-7d64560817e6"
      },
      "outputs": [],
      "source": [
        "#Write your code here, add you plot using ´plot_boxplot_violinplot´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbTUijRfJ14I"
      },
      "source": [
        "## Pregunta\n",
        "*¿Cómo crees que la eliminación de datos atípicos ha afectado la distribución y los patrones observados en las gráficas? ¿Qué cambios específicos puedes identificar en los datos después de esta eliminación?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thedrUeEPLfv"
      },
      "source": [
        "## Normalización y Escalado\n",
        "### Estandarización:\n",
        "Vamos a convertir los datos numéricos a una escala común. Para esto, vamos a aplicar la estandarización sobre las columnas numéricas \"Idade\" y \"Renda\". Luego, para ajustar la escala, vamos a utilizar las clases StandardScaler y ColumnTransformer. Debes implementar el código necesario para realizar la conversión.\n",
        "\n",
        "Para la clase DataScaleImputer, debes investigar un poco cómo realizar la conversión a una escala estándar (StandardScaler) mediante el uso de la clase ColumnTransformer. Puedes revisar la documentación oficial de sklearn.\n",
        "\n",
        "Otra opción es crear una función que permita estandarizar las características eliminando la media y escalando a varianza unitaria. Esto se calcula como: z = (x - u) / s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt_wI80YPTuz"
      },
      "outputs": [],
      "source": [
        "# Custom class to impute and scale data\n",
        "class DataScaleImputer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns  # Columns to be scaled\n",
        "        self.scaler = StandardScaler()  # Initialize the StandardScaler\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Fit the scaler on the specified columns\n",
        "        self.scaler.fit(X[self.columns])\n",
        "        return self  # Return the transformer\n",
        "\n",
        "    def transform(self, X):\n",
        "        data = X.copy()  # Make a copy of the input DataFrame to avoid modifying the original\n",
        "\n",
        "        # Create a ColumnTransformer that will apply StandardScaler only to the specified columns\n",
        "        # Write you code here, change None by custom transformer\n",
        "        transformer = None\n",
        "\n",
        "        # Apply the transformer to the data\n",
        "        X_transform = transformer.fit_transform(data)\n",
        "\n",
        "        # Convert the result to a DataFrame to maintain the column labels\n",
        "        X_imputed_df = pd.DataFrame(data=X_transform, columns=self.columns)\n",
        "\n",
        "        # Replace the original columns in 'data' with the scaled columns\n",
        "        data[self.columns] = X_imputed_df[self.columns]\n",
        "\n",
        "        return data.dropna()  # Return the transformed DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRs3f4KfKwK7"
      },
      "source": [
        "*Ejecuta la transformación utilizando la clase `DataScaleImputer` y asigna el resultado a `df_insurance`*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "9J6LZJFTPYt3",
        "outputId": "1833fa4b-d351-4ffe-9658-58d23477528d"
      },
      "outputs": [],
      "source": [
        "# Write you code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyee4DqsLASv"
      },
      "source": [
        "*Imprime las estadísticas básicas del conjunto de datos df_insurance, ubásicas utilizando el método `describe()`*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "rY20VS4S1Jb6",
        "outputId": "d1fe94be-c12d-49cf-b3a6-90c9fb788295"
      },
      "outputs": [],
      "source": [
        "# Write you code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOPESaM3TpYS"
      },
      "source": [
        "## Pregunta\n",
        "*¿Cuáles otras técnicas conoces que pueden ser utilizadas para escalar o normalizar los datos? Menciona dos.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmXreDFxOh_9"
      },
      "source": [
        "## Unificación de conjuntos de datos\n",
        "\n",
        "Vamos a unificar diferentes conjuntos de datos (`df_insurance`, `df_retailbank` y `df_investment`) para crear un nuevo DataFrame. Utilizaremos la función `merge` de Pandas, identificando previamente el atributo que nos permitirá integrar estos conjuntos como uno solo. El resultado final se asignará a la variable `data_frame_merged`. A continuación, mostraremos los primeros 10 registros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xntHRFe9NYKI"
      },
      "source": [
        "*Utiliza la función `merge` de Pandas para fusionar los conjuntos de datos en uno solo, asignándolo a la variable `data_frame_merged`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "hCoiBwAtOj3C",
        "outputId": "4fd62378-9c02-4297-885b-9ed22816c7d8"
      },
      "outputs": [],
      "source": [
        "# Write you code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3lgHpBiMkM3"
      },
      "source": [
        "*Imprime la cantidad total de registros después de realizar el merge entre los conjuntos de datos.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qq2s81fTLT_",
        "outputId": "e6a05ca0-d57e-4227-c147-ffcea1c790bc"
      },
      "outputs": [],
      "source": [
        "# Write you code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DDBqQTzne_"
      },
      "source": [
        "*Observamos una visión estadística rápida de los datos mediante la función `describe`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "RlODDsnh0ocW",
        "outputId": "ed794b02-db0b-4a67-fb4c-9e1fabe4ca32"
      },
      "outputs": [],
      "source": [
        "# Write you code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jh-D9BSzjNq"
      },
      "source": [
        "*Verifica si hay datos faltantes en el DataFrame resultante.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o9yUkEE0tg8",
        "outputId": "3423c7a4-b79e-4e0e-a540-ad9cb7ccc84b"
      },
      "outputs": [],
      "source": [
        "# Write you code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWbbNsAjvJzA"
      },
      "source": [
        "# Correcion nombres columnas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64ep6y1nN6fR"
      },
      "source": [
        "Como has notado, se presentan ciertos inconvenientes en los nombres de las columnas. A continuación, intentaremos resolver estos errores identificando y corrigiendo espacios adicionales u otros problemas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EyT3QlMvONh",
        "outputId": "3177636f-cd05-47cc-a017-9b18f97d9f88"
      },
      "outputs": [],
      "source": [
        "data_frame_merged = data_frame_merged.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "data_frame_merged.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG9C1cC81t0Z"
      },
      "source": [
        "## Finalización tramiento de datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBya2GKaRCgE"
      },
      "source": [
        "### Tratamiento de datos para modelos de Machine Learning\n",
        "\n",
        "Como último paso, es necesario ejecutar el siguiente tratamiento a los datos con el objetivo de prepararlos para nuestros modelos de Machine Learning. Seguiremos los siguientes pasos:\n",
        "\n",
        "1. **Creación de la variable a predecir:** Se creará una nueva columna llamada \"tipo_financiamiento\", que será la variable a predecir por nuestros modelos de Machine Learning. Esta columna representará el tipo de financiamiento, permitiendo identificar si corresponde a una casa, un carro, ambos o ninguno. El siguiente proceso se ejecutará sobre el `data_frame_merged`, generando un nuevo DataFrame llamado `data_frame_tipo_financiamiento` con la columna adicional \"tipo_financiamiento\".\n",
        "\n",
        "2. **Etiquetado de columnas categóricas:** Se etiquetarán las columnas categóricas como multi label. Las columnas identificadas son \"AGE_RANGE\", \"INCOME_RANGE\", \"tipo_financiamiento\" y \"Regiao\".\n",
        "\n",
        "3. **Eliminación de la columna \"ID\":** Se eliminará la columna utilizada como identificador.\n",
        "\n",
        "4. **Conversión de valores binarios:** Se convertirán todas las columnas con valores 'F' o 'T' a tipos de datos numéricos 0 y 1, respectivamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caufcNMephab"
      },
      "outputs": [],
      "source": [
        "class OneHotDecoderImputer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns, label_column_name):\n",
        "        \"\"\"\n",
        "        Initialize the OneHotDecoderImputer.\n",
        "\n",
        "        Parameters:\n",
        "        - columns: list of str, names of columns to be converted from one-hot encoding\n",
        "        - label_column_name: str, name of the new label column\n",
        "        \"\"\"\n",
        "        self.columns = columns  # List of column names to be converted from one-hot encoding\n",
        "        self.label_column_name = label_column_name\n",
        "        self.label_encoders = {}  # Dictionary to store label encoders for each column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the label encoders on the specified columns.\n",
        "\n",
        "        Parameters:\n",
        "        - X: pd.DataFrame, the input DataFrame\n",
        "\n",
        "        Returns:\n",
        "        - self: OneHotDecoderImputer, the transformer instance\n",
        "        \"\"\"\n",
        "        for col in self.columns:\n",
        "            encoder = LabelEncoder()\n",
        "            encoder.fit(X[col])\n",
        "            self.label_encoders[col] = encoder\n",
        "        return self  # Return the transformer instance\n",
        "\n",
        "    def get_financing_type_name_from_row(self, row):\n",
        "        \"\"\"\n",
        "        Get the financing type name from a one-hot encoded row.\n",
        "\n",
        "        Parameters:\n",
        "        - row: pd.Series, a row of one-hot encoded data\n",
        "\n",
        "        Returns:\n",
        "        - str or None, the name of the financing type or None if not found\n",
        "        \"\"\"\n",
        "        total_financing_types = row.sum()\n",
        "        if total_financing_types == len(row):\n",
        "            return \"Ambos\"\n",
        "        if total_financing_types == 0:\n",
        "            return \"Ninguno\"\n",
        "        for col_name, value in row.items():\n",
        "            if value == 1:\n",
        "                return col_name\n",
        "        return None\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Convert one-hot encoded columns to a single label column.\n",
        "\n",
        "        Parameters:\n",
        "        - X: pd.DataFrame, the input DataFrame\n",
        "\n",
        "        Returns:\n",
        "        - X_transformed: pd.DataFrame, the DataFrame with the new label column\n",
        "        \"\"\"\n",
        "        # Create a copy of the input DataFrame to keep the original data\n",
        "        X_transformed = X.copy()\n",
        "\n",
        "        # Encode the specified columns using the fitted label encoders\n",
        "        for col in self.columns:\n",
        "            X_transformed[col] = self.label_encoders[col].transform(X[col])\n",
        "\n",
        "        # Create the label column by applying the method to each row\n",
        "        X_transformed[self.label_column_name] = X_transformed[self.columns].apply(lambda row: self.get_financing_type_name_from_row(row), axis=1)\n",
        "\n",
        "        return X_transformed.drop(columns=self.columns)\n",
        "class BooleanToNumeric(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the BooleanToNumeric transformer.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the BooleanToNumeric transformer.\n",
        "\n",
        "        Parameters:\n",
        "        - X: pd.DataFrame, the input DataFrame\n",
        "\n",
        "        Returns:\n",
        "        - self: BooleanToNumeric, the transformer instance\n",
        "        \"\"\"\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform boolean values (\"T\" or \"F\") to numerical values (1 or 0).\n",
        "\n",
        "        Parameters:\n",
        "        - X: pd.DataFrame, the input DataFrame\n",
        "\n",
        "        Returns:\n",
        "        - X_transformed: pd.DataFrame, the transformed DataFrame\n",
        "        \"\"\"\n",
        "        X_transformed = X.replace({\"T\": 1, \"F\": 0})\n",
        "        return X_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu9NmrfdQG3r"
      },
      "outputs": [],
      "source": [
        "class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns=None):\n",
        "        \"\"\"\n",
        "        Initialize the MultiColumnLabelEncoder.\n",
        "\n",
        "        Parameters:\n",
        "        - columns: array of str, names of columns to encode. If None, encode all columns.\n",
        "        \"\"\"\n",
        "        self.columns = columns\n",
        "        self.label_encoders = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the label encoders on the specified columns.\n",
        "\n",
        "        Parameters:\n",
        "        - X: pd.DataFrame, the input DataFrame\n",
        "\n",
        "        Returns:\n",
        "        - self: MultiColumnLabelEncoder, the transformer instance\n",
        "        \"\"\"\n",
        "        if self.columns is None:\n",
        "            self.columns = X.columns\n",
        "        for col in self.columns:\n",
        "            self.label_encoders[col] = LabelEncoder().fit(X[col])\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform the specified columns using the fitted label encoders.\n",
        "\n",
        "        Parameters:\n",
        "        - X: pd.DataFrame, the input DataFrame\n",
        "\n",
        "        Returns:\n",
        "        - X_transformed: pd.DataFrame, the DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        X_transformed = X.copy()\n",
        "        for col in self.columns:\n",
        "            X_transformed[col] = self.label_encoders[col].transform(X[col])\n",
        "        return X_transformed\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit label encoders on the specified columns and transform the DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "        - X: pd.DataFrame, the input DataFrame\n",
        "\n",
        "        Returns:\n",
        "        - X_transformed: pd.DataFrame, the DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        \"\"\"\n",
        "        Reverse the encoding back to the original values.\n",
        "\n",
        "        Parameters:\n",
        "        - X: pd.DataFrame, the DataFrame with encoded columns\n",
        "\n",
        "        Returns:\n",
        "        - X_inverse: pd.DataFrame, the DataFrame with original values\n",
        "        \"\"\"\n",
        "        X_inverse = X.copy()\n",
        "        for col in self.columns:\n",
        "            X_inverse[col] = self.label_encoders[col].inverse_transform(X[col])\n",
        "        return X_inverse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qawwxcklQI3W"
      },
      "outputs": [],
      "source": [
        "class DropColumns(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns=None):\n",
        "        \"\"\"\n",
        "        Initialize the DropColumns transformer.\n",
        "\n",
        "        Parameters:\n",
        "        - columns: list of str, names of columns to drop from the DataFrame\n",
        "        \"\"\"\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the DropColumns transformer.\n",
        "\n",
        "        Parameters:\n",
        "        - X: pd.DataFrame, the input DataFrame\n",
        "\n",
        "        Returns:\n",
        "        - self: DropColumns, the transformer instance\n",
        "        \"\"\"\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform the input DataFrame by dropping specified columns.\n",
        "\n",
        "        Parameters:\n",
        "        - X: pd.DataFrame, the input DataFrame\n",
        "\n",
        "        Returns:\n",
        "        - X_transformed: pd.DataFrame, the transformed DataFrame\n",
        "        \"\"\"\n",
        "        X_transformed = X.drop(columns=self.columns, errors='ignore')\n",
        "        return X_transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lErZpQ6ZvSx"
      },
      "source": [
        "Definimos el pipeline para ajustar los datos al formato requerido para resolver el ejercicio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OJ0KhCFiuVs"
      },
      "outputs": [],
      "source": [
        "pipeline_data_preparation = Pipeline([\n",
        "    (\"one_hote_to_label\" ,OneHotDecoderImputer(columns=[\"FinanciamentoCasa\",\"FinanciamentoCarro\"], label_column_name = \"tipo_financiamiento\" )),\n",
        "    (\"label_encode\", MultiColumnLabelEncoder(columns=[\"AGE_RANGE\",\"INCOME_RANGE\",\"tipo_financiamiento\",\"Regiao\"])),\n",
        "    ('drop_columns', DropColumns(columns=[\"ID\"])),\n",
        "    ('boolean_numeric', BooleanToNumeric())\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-tIyMn4Z3g8"
      },
      "source": [
        "*Ejecuta el pipeline para ajustar los datos y asignarlos a la variable `data_frame_tipo_financiamiento`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "xXdb4cK1jQKX",
        "outputId": "fde14002-f4e8-4831-b8ef-ddc87d7142a8"
      },
      "outputs": [],
      "source": [
        "data_frame_tipo_financiamiento = pipeline_data_preparation.fit_transform(data_frame_merged)\n",
        "data_frame_tipo_financiamiento.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X54jFRjH5YBE"
      },
      "source": [
        "Obtenemos las etiquetas por tipo de financiamiento y asignamos a la varabile `le_tipo_financiamiento_mapping`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7KgPOVM5WSN",
        "outputId": "0597ccdf-e95a-40eb-e17f-abde9830b706"
      },
      "outputs": [],
      "source": [
        "le = pipeline_data_preparation[1].label_encoders[\"tipo_financiamiento\"]\n",
        "le_tipo_financiamiento_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "tipo_financiamiento_mapping = {v: k for k, v in le_tipo_financiamiento_mapping.items()}\n",
        "tipo_financiamiento_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSBEkrUUTg7R"
      },
      "source": [
        "*Imprime las estadísticas básicas del conjunto de datos `data_frame_tipo_financiamiento`*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "26INgJazTgXU",
        "outputId": "03712ddd-eb04-44b1-d9fc-abd46be41646"
      },
      "outputs": [],
      "source": [
        "# Write you code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyOz4_XcTQrJ"
      },
      "source": [
        "## Cierre tratamiento de datos\n",
        "Es crucial comprender que el tratamiento de datos no es solo una etapa preliminar, sino un proceso continuo que puede influir significativamente en el rendimiento y la precisión de los modelos de Machine Learning. Al abordar de manera efectiva problemas como valores faltantes, valores atípicos y errores de formato, estamos creando un conjunto de datos robusto y confiable, lo que a su vez potencia la capacidad predictiva de nuestros modelos.\n",
        "\n",
        "Hasta este punto, hemos completado varios pasos relacionados con el tratamiento y la limpieza de datos. Ahora vamos a continuar con el desarrollo de los diferentes algoritmos de Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgbOcCLdSctK"
      },
      "source": [
        "*Exporta el DataFrame data_frame_tipo_financiamiento a un archivo CSV sin incluir el índice*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMqBJdxLpNCY"
      },
      "outputs": [],
      "source": [
        "# Write you code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIYCwr8Ugmnf"
      },
      "source": [
        "# **Pregunta 3 - Creación de modelos de Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HAAt2nV2JVj"
      },
      "source": [
        "# Selección de modelo:\n",
        "Identifica el tipo de problema de aprendizaje (clasificación, regresión, agrupamiento, etc.) y selecciona los modelos más adecuados para tu problema.\n",
        "Experimenta con diferentes algoritmos de machine learning y ajusta sus hiperparámetros para encontrar la mejor combinación.\n",
        "1. Entrenamiento de modelos:\n",
        "Entrena por lo menos 3 modelos utilizando los datos de entrenamiento. Ajusta los parámetros del modelo utilizando algoritmos de optimización como la descenso del gradiente, búsqueda de cuadrícula, o búsqueda aleatoria.\n",
        "Valida el modelo utilizando los datos de validación y ajusta los parámetros según sea necesario para evitar el sobreajuste (overfitting).\n",
        "2. Evaluación del modelo:\n",
        "Evalúa el rendimiento del modelo utilizando métricas apropiadas para tu problema (precisión, recall, F1-score, matriz de confusión, etc.).\n",
        "Utiliza técnicas de validación cruzada para obtener estimaciones más robustas del rendimiento del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSqEhWR-euOX"
      },
      "source": [
        "## Pregunta\n",
        "*¿Cuál es el tipo de problema que estás enfrentando: clasificación o regresión? Imprime o grafica el conteo de valores que corresponde a la columna `data_frame_tipo_financiamiento`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKplgrw3fRYZ",
        "outputId": "ef17bd00-3727-4153-91bc-3d13c854c1dd"
      },
      "outputs": [],
      "source": [
        "# Write you code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "KlqLYwYJscX2",
        "outputId": "d777da8a-4059-4fc9-e0cb-8d155b2e72a0"
      },
      "outputs": [],
      "source": [
        "# Write you code here, add your custom plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aST1CQXfdrEX"
      },
      "source": [
        "## Pasos para el entrenamiento de modelos\n",
        "\n",
        "A continuación, desarrolla los siguientes pasos para cada uno de los modelos sobre el conjunto de datos `data_frame_tipo_financiamiento`:\n",
        "* **División del conjunto de datos:** Divide los datos en conjuntos de entrenamiento y prueba. El conjunto de entrenamiento se utiliza para entrenar el modelo, mientras que el conjunto de prueba se utiliza para evaluar su rendimiento.\n",
        "\n",
        "* **Selección de modelo:** Elige el algoritmo de aprendizaje automático más adecuado para tu problema. Esto depende del tipo de problema (regresión, clasificación, clustering, etc.), el tamaño y la naturaleza de los datos, y los requisitos de rendimiento.\n",
        "\n",
        "* **Entrenamiento del modelo:** Utiliza el conjunto de entrenamiento para ajustar los parámetros del modelo. Durante este proceso, el modelo aprenderá a mapear las características de entrada a las etiquetas de salida.\n",
        "\n",
        "* **Validación del modelo:** Evalúa el rendimiento del modelo utilizando el conjunto de prueba. Esto te permite verificar si el modelo generaliza bien a datos no vistos y detectar posibles problemas de sobreajuste o subajuste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYswcPCnR51B"
      },
      "source": [
        "### **Pasos para el entrenamiento del modelo - (Nombre Modelo)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrtVj7wBhvx7"
      },
      "outputs": [],
      "source": [
        "# Load your dataset\n",
        "# Assuming your data is stored in a DataFrame called 'data_frame_tipo_financiamiento'\n",
        "# and the target variable is in a column called 'tipo_financiamiento'\n",
        "# Replace 'data_frame_tipo_financiamiento' and 'tipo_financiamiento' with your actual DataFrame and column names\n",
        "# Write you code here\n",
        "\n",
        "# Split the data into training and testing sets using startified_train_test_split\n",
        "# You can adjust the test_size parameter as needed\n",
        "# 'random_state' ensures reproducibility of results\n",
        "# Write you code here\n",
        "\n",
        "# Create the custom model\n",
        "# You can customize the parameters based on your requirements\n",
        "# Write you code here\n",
        "\n",
        "# Train the model on the training data\n",
        "# Write you code here\n",
        "\n",
        "# Make predictions on the testing data\n",
        "# Write you code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-nY4h74qH5K"
      },
      "source": [
        "### **Evaluación del modelo - (Nombre Modelo)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl6jKYIGnswY",
        "outputId": "d41d5485-941b-48a2-8fd9-dc0f63192343"
      },
      "outputs": [],
      "source": [
        "# Evaluate accuracy the model\n",
        "# Write you code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "RNW6oOvZpTOE",
        "outputId": "fdce8c72-4610-4d9c-ae04-1af231bbcc83"
      },
      "outputs": [],
      "source": [
        "# Plot accuracy the model over the time - use plot_accuracy_scores\n",
        "#plot_accuracy_scores(rf_model,X_train,y_train,X_test,y_test,nparts=5,jobs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIhkkNlqj0HA",
        "outputId": "f72aaf87-c23f-4cf6-dfda-e2e0dbc86427"
      },
      "outputs": [],
      "source": [
        "# Print classifitacion report using classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "mt-8M4ndhQBu",
        "outputId": "9a1bf97d-a8ee-4256-e839-7228d9cc4ada"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix using plot_confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AntFMBx4haPQ"
      },
      "source": [
        "### **Pasos para el entrenamiento del modelo  a comparar - (LogisticRegression)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOz9js5_q_zp"
      },
      "outputs": [],
      "source": [
        "# Load your dataset\n",
        "# Assuming your data is stored in a DataFrame called 'data_frame_tipo_financiamiento'\n",
        "# and the target variable is in a column called 'tipo_financiamiento'\n",
        "# Replace 'data_frame_tipo_financiamiento' and 'tipo_financiamiento' with your actual DataFrame and column names\n",
        "X = data_frame_tipo_financiamiento.drop(columns=['tipo_financiamiento'])  # Features\n",
        "y = data_frame_tipo_financiamiento['tipo_financiamiento']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# You can adjust the test_size parameter as needed\n",
        "# 'random_state' ensures reproducibility of results\n",
        "X_train, X_test, y_train, y_test = startified_train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Random LogisticRegression\n",
        "# You can customize the parameters based on your requirements\n",
        "lr_model = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
        "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
        "                   random_state=1355, solver='lbfgs', tol=0.0001, verbose=0,\n",
        "                   warm_start=False)\n",
        "\n",
        "# Train the model on the training data\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = lr_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIrk0WL1rk1t"
      },
      "source": [
        "### **Evaluación del modelo - (LogisticRegression)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz21Ctr8rUN5",
        "outputId": "76655883-1307-4f5d-cfb0-359c9e722c94"
      },
      "outputs": [],
      "source": [
        "# Evaluate accuracy the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Gs6yfsJfrg1m",
        "outputId": "a051d47f-258c-4f2a-886d-a14002837384"
      },
      "outputs": [],
      "source": [
        "# Plot accuracy the model over the time\n",
        "plot_accuracy_scores(lr_model,X_train,y_train,X_test,y_test,nparts=5,jobs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG7luv94rszE",
        "outputId": "a306b569-5fa1-4ddc-ed67-1cfed40699b4"
      },
      "outputs": [],
      "source": [
        "# Print classifitacion report\n",
        "clas_report=classification_report(y_test,y_pred,labels=np.unique(y_pred), digits=6)\n",
        "print(clas_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "FI8Q-jDCrxCQ",
        "outputId": "4a017001-5316-415c-faac-b47031943843"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(confusion_matrix(y_test, y_pred),tipo_financiamiento_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNSapmt2hkRs"
      },
      "source": [
        "## Preguntas\n",
        "* *¿Puedes comparar los modelos y determinar cuál de ellos tiene un mejor rendimiento en términos de exactitud?*\n",
        "* *¿Logran los modelos etiquetar todas las clases de forma precisa? ¿Qué estrategias podrían aplicarse para mejorar este aspecto?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymc_ryzssy0E"
      },
      "source": [
        "## Extracción de características y Análisis de Componentes Principales(PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J34RIBP9wFS2"
      },
      "source": [
        "Aquí está la corrección:\n",
        "\n",
        "Ahora vamos a desarrollar validaciones para ver cuáles características son más relevantes para el modelo. Para esto, debes a implementar una función llamada  `plot_correlations` que te permita graficar las correlaciones del DataFrame `data_frame_tipo_financiamiento`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHoo0BWqwGst"
      },
      "outputs": [],
      "source": [
        "def plot_correlations(df_temp):\n",
        "    #Write your code here    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "g5RZ85ImwN-j",
        "outputId": "48f9e91f-5a99-4473-b97d-b92c1b4a2de2"
      },
      "outputs": [],
      "source": [
        "#Write your code here, plot using plot_correlations\n",
        "plot_correlations(data_frame_tipo_financiamiento)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3MfAWzguFQp"
      },
      "source": [
        "## Pregunta\n",
        "* *¿Puedes identificar cuáles columnas son más relevantes y por qué?*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFuo18ojuWQd"
      },
      "source": [
        "La siguiente función, `get_most_important_features`, nos permite extraer aquellas n columnas más relevantes a partir de la matriz de correlación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjA7etuy-ndX"
      },
      "outputs": [],
      "source": [
        "def get_most_important_features(correlation_matrix, target_column, n=5):\n",
        "    \"\"\"\n",
        "    Get the top N most important features based on their absolute correlation values.\n",
        "\n",
        "    Parameters:\n",
        "    - correlation_matrix: pd.DataFrame, the correlation matrix\n",
        "    - target_column: str, the name of the target variable column\n",
        "    - n: int, the number of top features to return\n",
        "\n",
        "    Returns:\n",
        "    - top_features: list, the top N most important feature names\n",
        "    \"\"\"\n",
        "    # Get the absolute correlation values with the target variable\n",
        "    correlation_with_target = correlation_matrix[target_column].abs().sort_values(ascending=False)\n",
        "\n",
        "    # Exclude the target variable itself\n",
        "    correlation_with_target = correlation_with_target.drop(target_column)\n",
        "\n",
        "    # Get the top N most important features\n",
        "    top_features = correlation_with_target.head(n).index.tolist()\n",
        "\n",
        "    return top_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiDn5loIu9rr"
      },
      "source": [
        "*Utiliza la función `get_most_important_features` e imprime las primeras 6 columnas más relevantes del conjunto de datos que están relacionadas con la variable a predecir.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDx_6Yz0-oSy",
        "outputId": "dd61bb27-3ec5-4ed5-c83b-15921941b9a6"
      },
      "outputs": [],
      "source": [
        "#Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzzz1GeOAYgJ"
      },
      "source": [
        "# Análisis de Componentes Principales(PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CswKzBbRTb6"
      },
      "source": [
        "## Pregunta\n",
        "*¿Qué es el análisis de componentes principales y cuál es su utilidad al implementar modelos de machine learning?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yf_iHC0R_hD"
      },
      "source": [
        "*Modifica la función `create_pca_model`. Los parámetros de entrada son el conjunto de datos sin la variable a predecir. Se creará un modelo de Análisis de Componentes Principales (PCA), el cual tendrá como parámetro el número N de componentes a identificar. El resultado será el modelo exportado y la transformación hacia las componentes principales luego de evaluar el modelo.*\n",
        "\n",
        "* Esta función toma como entrada `X_train`, que representa las características del conjunto de entrenamiento sin la variable objetivo, y `n_components`, que indica el número de componentes principales que se desea conservar.\n",
        "\n",
        "* Dentro de la función, se instancia un objeto PCA con el número de componentes especificado. Luego, se ajusta este objeto PCA a las características del conjunto de entrenamiento para determinar las características transformadas.\n",
        "\n",
        "* Estas características transformadas se almacenan en un DataFrame llamado `X_principal`, que luego se devuelve junto con el objeto PCA ajustado (`pca_model`) como salida de la función.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKa1QpN_R-lG"
      },
      "outputs": [],
      "source": [
        "def create_pca_model(X_train, n_components):\n",
        "    \"\"\"\n",
        "    Create a Principal Component Analysis (PCA) model.\n",
        "\n",
        "    Parameters:\n",
        "    X_train (DataFrame): The training dataset without the target variable.\n",
        "    n_components (int): The number of principal components to identify.\n",
        "\n",
        "    Returns:\n",
        "    pca_model (PCA): The fitted PCA model.\n",
        "    X_principal (DataFrame): The transformed features into principal components.\n",
        "    \"\"\"\n",
        "    # Instantiate PCA\n",
        "    #Write your code here\n",
        "    pca_model = None\n",
        "\n",
        "    # Fit PCA to the training data and transform features\n",
        "    #Write your code here\n",
        "    X_principal = None\n",
        "\n",
        "    # Return pca_model,X_principal\n",
        "    return pca_model,X_principal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMdoLPzWV4El"
      },
      "source": [
        "Llamamos a la función `create_pca_model`, pasando como argumentos el DataFrame `data_frame_tipo_financiamiento` y `n_components` igual a 10, para determinar las 10 componentes principales del conjunto de datos. Luego, graficamos la varianza acumulada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "yGAiU5L0V3CJ",
        "outputId": "ad6a3260-e0b8-4e03-db99-15491f56aa94"
      },
      "outputs": [],
      "source": [
        "pca_model,X_principal = create_pca_model(data_frame_tipo_financiamiento.drop(columns=['tipo_financiamiento']),n_components=10)\n",
        "plot_pca_cumulative_variance(pca_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnw50m2jZUgL"
      },
      "source": [
        "A continuación, obtenemos la lista de los N componentes principales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "COHvzFw_AfMh",
        "outputId": "355fb4e4-e763-424a-af10-24b4e4189119"
      },
      "outputs": [],
      "source": [
        "df_pca_components = get_pca_components(pca_model,data_frame_tipo_financiamiento.drop(columns=['tipo_financiamiento']).columns)\n",
        "df_pca_components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuPOo_lLZk48"
      },
      "source": [
        "## Pregunta\n",
        "*Compara las variables obtenidas después de realizar el PCA en el conjunto de datos con las variables identificadas a través de la matriz de confusión. ¿Has encontrado coincidencias entre las variables y qué conclusiones puedes extraer de esto?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrMMGQi2bJf9"
      },
      "source": [
        "Vamos a graficar la curva conocida como codo (elbow curve) utilizando la función `plot_elbow_curve_pca`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "x2ex4onvAjyS",
        "outputId": "ed9fdb35-2d7f-4315-e2d5-63d113fcf8b6"
      },
      "outputs": [],
      "source": [
        "plot_elbow_curve_pca(X_principal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMr5E8U1cEYC"
      },
      "source": [
        "## Pregunta\n",
        "*Primero, investiga para qué sirve la curva conocida como codo (elbow curve). Luego, responde a la pregunta: ¿Cuántos componentes principales (columnas) puedes sugerir que sean utilizados por algún modelo de Machine Learning?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSXDFPdUdHJH"
      },
      "source": [
        "*Establece el valor para la variable `n_components_pca`, luego ejecuta el modelo de aprendizaje, que incluye una tarea de reducción de la dimensionalidad mediante PCA (Análisis de Componentes Principales).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYvyAxxNdSmX"
      },
      "outputs": [],
      "source": [
        "n_componenets_pca = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m79FZskCGpd"
      },
      "outputs": [],
      "source": [
        "X = data_frame_tipo_financiamiento.drop(columns=['tipo_financiamiento'])\n",
        "y = data_frame_tipo_financiamiento['tipo_financiamiento']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the steps of the pipeline\n",
        "steps = [\n",
        "\n",
        "    ('pca', PCA(n_components=n_componenets_pca)),   # Apply PCA to reduce dimensionality to 2 components\n",
        "    ('clf', GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
        "                           learning_rate=0.1, loss='log_loss', max_depth=3,\n",
        "                           max_features=None, max_leaf_nodes=None,\n",
        "                           min_impurity_decrease=0.0, min_samples_leaf=1,\n",
        "                           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                           n_estimators=100, n_iter_no_change=None,\n",
        "                           random_state=447, subsample=1.0, tol=0.0001,\n",
        "                           validation_fraction=0.1, verbose=0,\n",
        "                           warm_start=False))  # Example classifier\n",
        "]\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0YkEhMhestv",
        "outputId": "e12fe791-72f6-48bd-8c38-05fcdbffaf00"
      },
      "outputs": [],
      "source": [
        "# Print classifitacion report\n",
        "clas_report=classification_report(y_test,y_pred,labels=np.unique(y_pred), digits=6)\n",
        "print(clas_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "yNBcEifDejU1",
        "outputId": "3a8cedc6-4ade-4b12-973a-40056a8c061c"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(confusion_matrix(y_test, y_pred),tipo_financiamiento_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvKmAwE1wipb"
      },
      "source": [
        "Interaction Terms:\n",
        "\n",
        "Create new features that capture the interactions between existing features. For instance, combining pairs of binary features using logical operations like AND, OR might uncover useful patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWQLrhobPrTd"
      },
      "source": [
        "## Implementación del tratamiento de datos desbalanceados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQQ0IZkcALg0"
      },
      "outputs": [],
      "source": [
        "X = data_frame_tipo_financiamiento.drop(columns=['tipo_financiamiento'])\n",
        "y = data_frame_tipo_financiamiento['tipo_financiamiento']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Gs3PyzP5V1"
      },
      "source": [
        "A continuación, se muestra una gráfica que ilustra la presencia de datos desbalanceados.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "2PFlZFAKCdjm",
        "outputId": "ce54f600-7594-4bef-88fb-640114065c58"
      },
      "outputs": [],
      "source": [
        "conteo_tipo_financiamiento_label = y.value_counts().rename(index=tipo_financiamiento_mapping)\n",
        "conteo_tipo_financiamiento_label.plot.pie()\n",
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exEOAPsTQTwW"
      },
      "source": [
        "Para abordar el problema, vamos a comenzar reduciendo la variable que tiene mayor presencia y luego crearemos nuevos datos sintéticos para que los datos con menor presencia tengan la misma representatividad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72CJqAWv_wVe"
      },
      "outputs": [],
      "source": [
        "# Define the steps of the pipeline\n",
        "# Calculating class counts\n",
        "class_counts = data_frame_tipo_financiamiento['tipo_financiamiento'].value_counts()\n",
        "\n",
        "# Setting sampling strategy\n",
        "sampling_strategy = {3:int(class_counts.max() * 0.30), 1:int(class_counts[1]*0.20)}\n",
        "\n",
        "# Undersampling with RandomUnderSampler\n",
        "rand_under = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
        "\n",
        "# Oversampling with SMOTE\n",
        "smote_over = SMOTE(sampling_strategy='not majority', k_neighbors=5, random_state=42)\n",
        "\n",
        "# Steps for addressing imbalance\n",
        "steps_imbalance = [\n",
        "    ('sampling_under', rand_under),\n",
        "    ('sampling', smote_over)\n",
        "]\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline_fix_imbalance = Pipeline(steps_imbalance)\n",
        "pipeline_fix_imbalance.fit(X, y)\n",
        "\n",
        "# Resample the data\n",
        "X_reshaped, y_reshaped = pipeline_fix_imbalance.fit_resample(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yeg1DlZQxTN"
      },
      "source": [
        "*Implementa un gráfico tipo pie que muestre cómo lucen los datos después de realizar el tratamiento para abordar el desbalance.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "ma3XaCAgQs9n",
        "outputId": "807caa35-a73c-40ba-ee41-ce933ba4f9cb"
      },
      "outputs": [],
      "source": [
        "#Write your code here\n",
        "#conteo_tipo_financiamiento_label = y_reshaped.value_counts().rename(index=tipo_financiamiento_mapping)\n",
        "#conteo_tipo_financiamiento_label.plot.pie()\n",
        "#y_reshaped.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKehhQVcRZqW"
      },
      "source": [
        "*Separa los datos en conjuntos de entrenamiento y test utilizando la función `startified_train_test_split()`. Luego, implementa un modelo que haga uso del siguiente clasificador. Puedes probar modificando los hiperparámetros y evaluar los resultados. También puedes optar por modificar los parámetros de las clases `RandomUnderSampler` y `SMOTE` del paso anterior.*\n",
        "\n",
        "\n",
        "```\n",
        "GradientBoostingClassifier(\n",
        "        ccp_alpha=0.0,\n",
        "        criterion='friedman_mse',\n",
        "        init=None,\n",
        "        learning_rate=0.1,\n",
        "        loss='log_loss',\n",
        "        max_depth=3,\n",
        "        max_features=None,\n",
        "        max_leaf_nodes=None,\n",
        "        min_impurity_decrease=0.0,\n",
        "        min_samples_leaf=1,\n",
        "        min_samples_split=2,\n",
        "        min_weight_fraction_leaf=0.0,\n",
        "        n_estimators=100,\n",
        "        n_iter_no_change=None,\n",
        "        random_state=8860,\n",
        "        subsample=1.0,\n",
        "        tol=0.0001,\n",
        "        validation_fraction=0.1,\n",
        "        verbose=0,\n",
        "        warm_start=False)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWvRSLwn4ulZ"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets with stratification\n",
        "# Stratification ensures that the class distribution is preserved in both training and testing sets\n",
        "# Write your code here\n",
        "\n",
        "\n",
        "# Define the steps for the pipeline\n",
        "steps_gradient_boost = [\n",
        "    ('sampling_under', rand_under),  # Undersampling\n",
        "    ('sampling_over', smote_over),    # Oversampling\n",
        "    ('clf', GradientBoostingClassifier(\n",
        "        ccp_alpha=0.0,\n",
        "        criterion='friedman_mse',\n",
        "        init=None,\n",
        "        learning_rate=0.1,\n",
        "        loss='log_loss',\n",
        "        max_depth=3,\n",
        "        max_features=None,\n",
        "        max_leaf_nodes=None,\n",
        "        min_impurity_decrease=0.0,\n",
        "        min_samples_leaf=1,\n",
        "        min_samples_split=2,\n",
        "        min_weight_fraction_leaf=0.0,\n",
        "        n_estimators=100,\n",
        "        n_iter_no_change=None,\n",
        "        random_state=8860,\n",
        "        subsample=1.0,\n",
        "        tol=0.0001,\n",
        "        validation_fraction=0.1,\n",
        "        verbose=0,\n",
        "        warm_start=False))  # Example classifier\n",
        "]\n",
        "\n",
        "# Create the pipeline for Gradient Boosting\n",
        "# Write your code here\n",
        "\n",
        "# Train the model using fit\n",
        "# Write your code here\n",
        "\n",
        "# Make predictions\n",
        "# Write your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuKGvV6OSshT"
      },
      "source": [
        "Evaluemos los resultados del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9oNtyLcNRo3",
        "outputId": "6a88b273-ce0c-41e8-c99a-01d6199dae0d"
      },
      "outputs": [],
      "source": [
        "# Print classifitacion report\n",
        "clas_report=classification_report(y_test,y_pred,labels=np.unique(y_pred), digits=6)\n",
        "print(clas_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "fNx3W1QmEwHB",
        "outputId": "e723fbbd-1117-4083-b77a-402e2705e691"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(confusion_matrix(y_test, y_pred),tipo_financiamiento_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvd9HmHDNnst"
      },
      "source": [
        "# Pregunta 4\n",
        "* *¿Cuál de los modelos consideras que es más eficiente en términos de rendimiento y por qué?*\n",
        "* *Luego de evaluar los diferentes modelos, como científico de datos, ¿cuál sugerirías implementar y por qué? Justifica tu respuesta.*\n",
        "* *Investiga qué otras opciones pueden ser utilizadas para enfrentar el problema de datos desbalanceados e implementa un ejemplo.*\n",
        "* *Investiga qué son los modelos de ensamble e implementa un corto ejemplo.*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
